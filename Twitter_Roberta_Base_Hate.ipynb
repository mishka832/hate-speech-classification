{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3Jn8F9WmA_2"
      },
      "outputs": [],
      "source": [
        "# -------------------- IMPORTS --------------------\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.nn import CrossEntropyLoss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn torch --quiet"
      ],
      "metadata": {
        "id": "da23F_RjnUYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- DATA LOADING --------------------\n",
        "df = pd.read_csv('hate_sample_for_finetune.csv')"
      ],
      "metadata": {
        "id": "K4PA8anSmZW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['hate_type_teacher'])"
      ],
      "metadata": {
        "id": "oKfobfV7mbMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map text labels to integers\n",
        "label_mapping = {label: idx for idx, label in enumerate(df['hate_type_teacher'].unique())}\n",
        "df['hate_type_teacher_int'] = df['hate_type_teacher'].map(label_mapping)\n",
        "print(\"Label mapping:\", label_mapping)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w845J_mYmcei",
        "outputId": "24a40788-ca03-48c2-fdd3-0872d8ac452d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label mapping: {'political framing hate': 0, 'sarcasm-based hate': 1, 'meme-language hate': 2, 'humor-based hate': 3, 'metaphor-based hate': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- TRAIN-TEST SPLIT --------------------\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['clean_text'], df['hate_type_teacher_int'], test_size=0.1, random_state=42, stratify=df['hate_type_teacher_int']\n",
        ")\n"
      ],
      "metadata": {
        "id": "RF0mhqeLmcg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- TOKENIZATION --------------------\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-hate\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_mapping), ignore_mismatched_sizes=True)\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=256)\n",
        "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWU1mkvfmcjV",
        "outputId": "6512015d-b7a0-4bd5-dd8c-c74b02ff01ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-hate and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- CUSTOM DATASET --------------------\n",
        "class HateSpeechDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels.reset_index(drop=True)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(int(self.labels.iloc[idx]), dtype=torch.long)\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = HateSpeechDataset(train_encodings, train_labels)\n",
        "val_dataset = HateSpeechDataset(val_encodings, val_labels)\n"
      ],
      "metadata": {
        "id": "gqvi-sS4mclu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- WEIGHTED TRAINER --------------------\n",
        "# Compute class weights inversely proportional to frequency\n",
        "label_counts = df['hate_type_teacher_int'].value_counts().sort_index()\n",
        "class_weights = torch.tensor(1.0 / label_counts.values, dtype=torch.float)\n",
        "class_weights = class_weights / class_weights.sum() * len(label_counts)  # Normalize\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = CrossEntropyLoss(weight=class_weights.to(model.device))\n",
        "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I66t2yxAmcoL",
        "outputId": "a93ea6e3-f276-4e61-f53a-fc1e1c636734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights: tensor([1.6748, 0.1467, 1.0444, 1.5364, 0.5977])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- TRAINING ARGS --------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,  # you can reduce to 2 if GPU time is an issue\n",
        "    per_device_train_batch_size=8,  # smaller batch for less memory\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\", # Removed due to TypeError in current environment\n",
        "    # logging_strategy=\"epoch\",    # Removed due to TypeError in current environment\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    report_to=[]\n",
        ")"
      ],
      "metadata": {
        "id": "LHDj4Hf0mcqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- TRAINING --------------------\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "DwKiikLFmcsv",
        "outputId": "90f85610-5bbd-42ac-9f5d-ade3212f324b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='672' max='672' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [672/672 04:33, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.495937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.234332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.353600</td>\n",
              "      <td>1.296630</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=672, training_loss=1.2625169526963007, metrics={'train_runtime': 274.5595, 'train_samples_per_second': 19.548, 'train_steps_per_second': 2.448, 'total_flos': 706077535302144.0, 'train_loss': 1.2625169526963007, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------- SAVE MODEL --------------------\n",
        "model.save_pretrained('./twitter-roberta-hate-subtypes')\n",
        "tokenizer.save_pretrained('./twitter-roberta-hate-subtypes')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLkv8G6bmcvC",
        "outputId": "306abde4-7639-4664-84a2-20e6423b7aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./twitter-roberta-hate-subtypes/tokenizer_config.json',\n",
              " './twitter-roberta-hate-subtypes/special_tokens_map.json',\n",
              " './twitter-roberta-hate-subtypes/vocab.json',\n",
              " './twitter-roberta-hate-subtypes/merges.txt',\n",
              " './twitter-roberta-hate-subtypes/added_tokens.json',\n",
              " './twitter-roberta-hate-subtypes/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- EVALUATION --------------------\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "predictions = trainer.predict(val_dataset)\n",
        "pred_labels = predictions.predictions.argmax(axis=1)\n",
        "true_labels = predictions.label_ids\n",
        "\n",
        "print(\"\\nâœ… Accuracy:\", accuracy_score(true_labels, pred_labels))\n",
        "print(\"\\nðŸ“Š Classification Report:\\n\", classification_report(true_labels, pred_labels, target_names=label_mapping.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "uRYK1tn3mcxq",
        "outputId": "41b874b2-d61e-424c-c7a9-af1f5a281af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Accuracy: 0.5326633165829145\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "political framing hate       0.35      0.55      0.43        11\n",
            "    sarcasm-based hate       0.79      0.60      0.68       127\n",
            "    meme-language hate       0.32      0.44      0.37        18\n",
            "      humor-based hate       0.20      0.33      0.25        12\n",
            "   metaphor-based hate       0.29      0.39      0.33        31\n",
            "\n",
            "              accuracy                           0.53       199\n",
            "             macro avg       0.39      0.46      0.41       199\n",
            "          weighted avg       0.61      0.53      0.56       199\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sTPN-Glhmczz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cxAwUDHOmc17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJVB6zZUmc5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CFtOwe6Bmc70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gt9CK_wqmc9x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}